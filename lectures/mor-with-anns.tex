\documentclass[%
% aspectratio=1610
% aspectratio=169
% aspectratio=149
% aspectratio=141
% aspectratio=54
% aspectratio=32
]{beamer}

%\usetheme{pymor}                      % use this for section-based progress bar
\usetheme[globalbar, no-section-outline]{pymor}          % use this for global  progress bar
%\usetheme[staticbar]{pymor}          % use this for fixed "progress" bar
%\usetheme[no-section-outline]{pymor} % use to suppress automatic outline slides
                                      % this option can be combined with the
                                      % above

\usepackage{booktabs}
% \usepackage[scale=2]{ccicons}
\usepackage{minted}
\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}

\usepackage{tikz}
\usetikzlibrary{calc,positioning}

\usepackage{algorithm2e}

\usemintedstyle{trac}

\title[Model order reduction using artificial neural networks]{Model order reduction using\\artificial neural networks}
\subtitle{}

\date{October 7, 2021 -- Afternoon session}
\author[Hendrik  Kleikamp~(\href{mailto://hendrik.kleikamp@uni-muenster.de}{\ttfamily hendrik.kleikamp@uni-muenster.de})]{Hendrik Kleikamp}
\institute{Institute for Analysis and Numerics - WWU Münster}

\begin{document}

\maketitle
{%
  \setbeamercolor{background canvas}{bg=pymor_blue}
  \setbeamertemplate{frametitle}[plain]
  \setbeamercolor{subsection in toc}{fg=PaleGray, bg=pymor_blue}
  \setbeamertemplate{subsection in toc}
  {\leavevmode\leftskip=2em$\bullet$\hskip1em\inserttocsubsection\par}
 
%\begin{frame}[plain]{Outline}
%  \LARGE\tableofcontents
%\end{frame}
}

\definecolor{green}{HTML}{31a031}
\definecolor{red}{HTML}{a03131}

\def\vdist{1.45cm}
\def\hdist{2.8cm}

\def\customgray{lightgray}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
	\frametitle{Mathematical background}
	
	\begin{itemize}
		\item \emph{Starting point:} Given a full-order model $\mu\mapsto u(\mu)$ and a reduced space $V_N$ with orthonormal basis $\Psi_N$, how to compute a fast and reliable reduced model without affine decomposition of operators (i.e.\ without using EIM for instance)?
		\vfill
		\item<2-> \emph{Goal:} Approximate the map $\pi_N\colon\mathcal{P}\rightarrow\mathbb{R}^N$, given as
		\[
			\pi_N(\mu)=u_{\mathrm{proj}}(\mu)\in\mathbb{R}^N,
		\]
		where $u_{\mathrm{proj}}(\mu)$ holds the coefficients of the \textbf{orthogonal} projection of the full-order solution $u(\mu)$ onto the reduced space $V_N$.
		\vfill
		\item<3-> \emph{Idea:} Use a neural network to approximate $\pi_N$.
	\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Artificial neural networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
	\frametitle{Example: Feed-forward neural networks}

	\begin{tikzpicture}[->, >=latex, node distance=\vdist and \hdist, on grid]
		\tikzstyle{neuron} = [draw, circle];
		\tikzstyle{bias} = [draw, circle];
		\tikzstyle{annot} = [text width=4em, text centered];
		
		\node[neuron](x1) {$x_1$};
		\node[neuron, below=of x1](x2) {$x_2$};
		
		\node[neuron, right=of x1, yshift=.5*\vdist](h11) {$\rho$};
		\node[neuron, below=of h11](h12) {$\rho$};
		\node[neuron, below=of h12](h13) {$\rho$};
		
		\node[neuron, right=of h11, yshift=.5*\vdist](h21) {$\rho$};
		\node[neuron, below=of h21](h22) {$\rho$};
		\node[neuron, below=of h22](h23) {$\rho$};
		\node[neuron, below=of h23](h24) {$\rho$};
		
		\node[bias, below=of h24](b3) {$1$};
		\node[bias, left=of b3](b2) {$1$};
		\node[bias, left=of b2](b1) {$1$};
		
		\node[neuron, right=of h21, yshift=-.5*\vdist](o1) {$o_1$};
		\node[neuron, below=of o1](o2) {$o_2$};
		\node[neuron, below=of o2](o3) {$o_3$};
		
		\node[annot, above=1.5cm of h11](h1) {Hidden layer};
		\node[annot, left=of h1](i) {Input layer};
		\node[annot, right=of h1](h2) {Hidden layer};
		\node[annot, right=of h2](o) {Output layer};
		
		\draw (x1) -- (h11);
		\draw (x1) -- (h12);
		\draw (x1) -- (h13);
		
		\draw (b1) -- (h11);
		\draw (b1) -- (h12);
		\draw (b1) -- (h13);
		
		\draw (x2) -- (h11);
		\draw (x2) -- (h12);
		\draw (x2) -- (h13);
		
		\draw (h11) -- (h21);
		\draw (h11) -- (h22);
		\draw (h11) -- (h23);
		\draw (h11) -- (h24);
		
		\draw (h12) -- (h21);
		\draw (h12) -- (h22);
		\draw (h12) -- (h23);
		\draw (h12) -- (h24);
		
		\draw (h13) -- (h21);
		\draw (h13) -- (h22);
		\draw (h13) -- (h23);
		\draw (h13) -- (h24);
		
		\draw (b2) -- (h21);
		\draw (b2) -- (h22);
		\draw (b2) -- (h23);
		\draw (b2) -- (h24);
		
		\draw (h21) -- (o1);
		\draw (h21) -- (o2);
		\draw (h21) -- (o3);
		
		\draw (h22) -- (o1);
		\draw (h22) -- (o2);
		\draw (h22) -- (o3);
		
		\draw (h23) -- (o1);
		\draw (h23) -- (o2);
		\draw (h23) -- (o3);
		
		\draw (h24) -- (o1);
		\draw (h24) -- (o2);
		\draw (h24) -- (o3);
		
		\draw (b3) -- (o1);
		\draw (b3) -- (o2);
		\draw (b3) -- (o3);
		
		\node[annot, left=1.5cm of b1](b) {Bias neurons};
	\end{tikzpicture}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
	\frametitle{Definition: Feed-forward neural networks}
	Components of a neural network $\Phi$:
	\begin{itemize}
		\item Number of layers $L$
		\item Number of neurons $N_1,\dots,N_{L-1}$ in the $L-1$ hidden layers
		\item Input size $N_0$; Output size $N_L$
		\item Matrices $A_1,\dots,A_L$ with $A_i\in\mathbb{R}^{N_i\times N_{i-1}}$ (weights)
		\item Vectors $b_1,\dots,b_L$ with $b_i\in\mathbb{R}^{N_i}$ (biases)
		\item Activation function $\rho\colon\mathbb{R}\to\mathbb{R}$ (ReLU, $\tanh$, \dots)
	\end{itemize}
	\onslide<2->{
	For an input $x\in\mathbb{R}^{N_0}$, the output of $\Phi$ is given by
	\begin{align*}
		\Phi(x)&=A_Lr^{L-1}(x)+b_L,
	\end{align*}
	where
	\begin{align*}
		r^i(x)&=\rho^\ast(A_ir^{i-1}(x)+b_i),\qquad i=1,\dots,L-1,\\
		r^0(x)&=x,
	\end{align*}
	with $\rho^\ast$ being the component-wise application of $\rho$.}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
	\frametitle{Visualization of the definition: Feed-forward neural networks}

	\begin{tikzpicture}[->, >=latex, node distance=\vdist and \hdist, on grid]
		\tikzstyle{neuron} = [draw, circle];
		\tikzstyle{bias} = [draw, circle];
		\tikzstyle{annot} = [text width=4em, text centered];
		
		\node[neuron](x1) {$r_1^0(x)$};
		\node[neuron, below=of x1](x2) {$r_2^0(x)$};
		
		\node[neuron, right=of x1, yshift=.5*\vdist](h11) {$\rho$};
		\node[neuron, below=of h11](h12) {$\rho$};
		\node[neuron, below=of h12](h13) {$\rho$};
		
		\node[neuron, right=of h11, yshift=.5*\vdist](h21) {$\rho$};
		\node[neuron, below=of h21](h22) {$\rho$};
		\node[neuron, below=of h22](h23) {$\rho$};
		\node[neuron, below=of h23](h24) {$\rho$};
		
		\node[bias, below=of h24](b3) {$1$};
		\node[bias, left=of b3](b2) {$1$};
		\node[bias, left=of b2](b1) {$1$};
		
		\node[neuron, right=of h21, yshift=-.5*\vdist](o1) {$r_1^3(x)$};
		\node[neuron, below=of o1](o2) {$r_2^3(x)$};
		\node[neuron, below=of o2](o3) {$r_3^3(x)$};
		
		\node[annot, above=1.5cm of h11](h1) {Hidden layer};
		\node[annot, left=of h1](i) {Input layer};
		\node[annot, right=of h1](h2) {Hidden layer};
		\node[annot, right=of h2](o) {Output layer};
		
		\draw[draw=\customgray] (x1) -- (h11);
		\draw[draw=\customgray] (x1) -- (h12);
		\draw[draw=\customgray] (x1) -- (h13);
		
		\draw[draw=\customgray] (b1) -- (h11);
		\draw[draw=\customgray] (b1) -- (h12);
		
		\draw[draw=\customgray] (h12) -- (h21);
		\draw[draw=\customgray] (h12) -- (h22);
		\draw[draw=\customgray] (h12) -- (h23);
		\draw[draw=\customgray] (h12) -- (h24);
		
		\draw[draw=\customgray] (h13) -- (h21);
		\draw[draw=\customgray] (h13) -- (h23);
		\draw[draw=\customgray] (h13) -- (h24);
		
		\draw[draw=\customgray] (h11) -- (h23);
		\draw[draw=\customgray] (h11) -- (h24);
		
		\draw[draw=\customgray] (b2) -- (h21);
		\draw[draw=\customgray] (b2) -- (h22);
		\draw[draw=\customgray] (b2) -- (h23);
		
		\draw[draw=\customgray] (h22) -- (o1);
		\draw[draw=\customgray] (h22) -- (o2);
		\draw[draw=\customgray] (h22) -- (o3);
		
		\draw[draw=\customgray] (h23) -- (o1);
		\draw[draw=\customgray] (h23) -- (o2);
		\draw[draw=\customgray] (h23) -- (o3);
		
		\draw[draw=\customgray] (h24) -- (o1);
		\draw[draw=\customgray] (h24) -- (o2);
		\draw[draw=\customgray] (h24) -- (o3);
		
		\draw[draw=\customgray] (b3) -- (o1);
		\draw[draw=\customgray] (b3) -- (o2);
		
		\draw (b1) -- (h13) node[pos=0.65, fill=white, inner sep=1pt] {\scriptsize $(b_1)_3$};
		
		\draw (x2) -- (h11) node[pos=0.65, fill=white, inner sep=1pt] {\scriptsize $(A_1)_{1,2}$};
		\draw (x2) -- (h12) node[pos=0.65, fill=white, inner sep=1pt] {\scriptsize $(A_1)_{2,2}$};
		\draw (x2) -- (h13) node[pos=0.65, fill=white, inner sep=1pt] {\scriptsize $(A_1)_{3,2}$};
		
		\draw (h13) -- (h22) node[pos=0.65, fill=white, inner sep=1pt] {\scriptsize $(A_2)_{2,3}$};
		
		\draw (h11) -- (h21) node[pos=0.65, fill=white, inner sep=1pt] {\scriptsize $(A_2)_{1,1}$};
		\draw (h11) -- (h22) node[pos=0.65, fill=white, inner sep=1pt] {\scriptsize $(A_2)_{2,1}$};
		
		\draw (b2) -- (h24) node[pos=0.65, fill=white, inner sep=1pt] {\scriptsize $(b_2)_4$};
		
		\draw[draw=\customgray] (h21) -- (o3);
		\draw (h21) -- (o1) node[pos=0.65, fill=white, inner sep=1pt] {\scriptsize $(A_3)_{1,1}$};
		\draw (h21) -- (o2) node[pos=0.65, fill=white, inner sep=1pt] {\scriptsize $(A_3)_{2,1}$};
		
		\draw (b3) -- (o3) node[pos=0.65, fill=white, inner sep=1pt] {\scriptsize $(b_3)_3$};
		
		\node[annot, left=1.5cm of b1](b) {Bias neurons};
	\end{tikzpicture}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
	\frametitle{Neural network training in a nutshell}
	\begin{itemize}
		\item \emph{Task:} Adjust weights and biases such that $\Phi$ approximates a function $f$.
		\item \emph{Problem:} Typically, $f$ is not given explicitly, but we can only compute samples of $f$.
		\item<2-> \emph{Idea:} Choose training set $x_1,\dots,x_n$, compute $f(x_1),\dots,f(x_n)$, and minimize error
		\[
			E(\Phi) = \frac{1}{n}\sum\limits_{i=1}^{n}l(\Phi(x_i),f(x_i))
		\]
		with respect to the weights and biases of $\Phi$.\newline
		Here, $l\colon\mathbb{R}^{N_L}\times\mathbb{R}^{N_L}\to\mathbb{R}$ denotes a loss function, for instance $l(y,\tilde{y})=\lVert y-\tilde{y}\rVert_2^2$.
		\item<2-> \emph{Optimization of $E$:} Compute gradient of $E$ with respect to weights and biases using backpropagation and apply (variant of) gradient descent algorithm to gradually minimize $E$.
	\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
	\frametitle{Pseudo code for neural network training}
	\IncMargin{1.5em}
	\begin{algorithm}[H]
		\SetAlgoLined
		\SetNoFillComment
		\For{$r=1,\dots,N_\mathrm{restarts}$}
		{
			initialize weights and biases randomly\\
			\For{$e=1,\dots,N_\mathrm{epochs}$}
			{
				create batches of training data\\
				\tcc{Training}
				\For{$b=1,\dots,N_\mathrm{batches}$}
				{
					apply optimization step with loss function for batch $b$
				}
				\tcc{Validation}
				compute validation loss\\
				\If{early stopping criterion is fulfilled}
				{
					stop training and perform next restart
				}
			}
			update neural network with lowest validation loss if necessary
		}
	\end{algorithm}
	\DecMargin{1.5em}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation in pyMOR}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
	\frametitle{General information on the implementation in pyMOR}
	\begin{itemize}
		\item Reduced basis $\Psi_N$ via POD of snapshots $u(\mu_1),\dots,u(\mu_{n})$.
		\item Neural networks implemented and trained using PyTorch (\url{https://pytorch.org/}).
		\item Training with snapshots $u(\mu_i)$ projected on reduced space $V_N$:
		\[
			\{(\mu_i,\underbrace{u_{\mathrm{proj}}(\mu_i)}_{=\pi_N(u(\mu_i))})\in\mathcal{P}\times\mathbb{R}^N:i=1,\dots,n\}
		\]
		\item Validation phase to assess generalization ability.
		\item Early stopping and multiple restarts of training.
		\item Customizable training routine (optimizer, epochs, learning rate, \dots).
		\item \emph{Everything hidden in a reductor!}
	\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
	\frametitle{Let's look at some code \textellipsis}
	\begin{minted}[
	frame=lines,
	framesep=1.5mm,
	baselinestretch=1.1,
	fontsize=\scriptsize,
	]{python}
class NeuralNetworkReductor(BasicObject):
    def __init__(self, fom, training_set, validation_set=None,
                 validation_ratio=0.1, basis_size=None,
                 rtol=0., atol=0., l2_err=0., pod_params=None,
                 ann_mse='like_basis'):
        ...

    def reduce(self, hidden_layers='[(N+P)*3, (N+P)*3]',
               activation_function=torch.tanh,
               optimizer=optim.LBFGS, epochs=1000,
               batch_size=20, learning_rate=1., restarts=10,
               seed=0):
        ...
                                
    def reconstruct(self, u):
        ...
	\end{minted}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
	\frametitle{A bit more code \textellipsis}
	\begin{minted}[
	frame=lines,
	framesep=1.5mm,
	baselinestretch=1.1,
	fontsize=\scriptsize,
	]{python}
def train_neural_network(training_data, validation_data,
                         neural_network, training_parameters={}):
    """Training algorithm for artificial neural networks.

    Trains a single neural network using the given training and
    validation data."""
    ...

def multiple_restarts_training(training_data, validation_data,
                               neural_network, target_loss=None,
                               max_restarts=10,
                               training_parameters={}, seed=None):
    """Algorithm that performs multiple restarts of neural network
       training.

    This method either performs a predefined number of restarts and
    returns the best trained network or tries to reach a given target
    loss and stops training when the target loss is reached."""
    ...
	\end{minted}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
	\frametitle{The \mintinline{python}{NeuralNetworkReductor} in action}
	\begin{minted}[
	frame=lines,
	framesep=1.5mm,
	baselinestretch=1.1,
	fontsize=\scriptsize,
	]{python}
from pymor.basic import *
# Set up the problem and the full-order model
problem = StationaryProblem(...)
fom, _ = discretize_stationary_cg(problem)
parameter_space = fom.parameters.space(...)
# Sample randomly for training and test set
training_set = parameter_space.sample_uniformly(...)
validation_set = parameter_space.sample_randomly(...)
# Set up reductor and compute the reduced-order model
from pymor.reductors.neural_network import NeuralNetworkReductor
reductor = NeuralNetworkReductor(fom, training_set, validation_set,
                                 l2_err=..., ann_mse=...)
rom = reductor.reduce(restarts=...)
	\end{minted}
	\emph{More details:}
	\href{https://docs.pymor.org/main/tutorial_mor_with_anns.html}{\ttfamily\footnotesize https://docs.pymor.org/main/tutorial\_mor\_with\_anns.html}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
	\frametitle{Variants of the \texttt{NeuralNetworkReductor}}
	\emph{Instationary models:}\newline
	Treat time as an additional parameter and apply same procedure as for stationary problems, i.e.\ approximate the map $(\mu,t)\mapsto u_N(\mu,t)\in\mathbb{R}^N$ by a neural network.\newline
	\structure{Usage shown in a demo:}
	\href{https://github.com/pymor/pymor/blob/2021.1.x/src/pymordemos/neural_networks_instationary.py}{\ttfamily\footnotesize https://github.com/pymor/pymor/blob/2021.1.x/src/pymordemos/\\neural\_networks\_instationary.py}
	\vfill
	\onslide<2->{\emph{Statefree outputs:}\newline
	Learn map from parameter space to output directly without computing a (reduced) state, i.e.\ given an output functional $\mathcal{J}(\mu):=J(u(\mu),\mu)$, approximate $\mathcal{J}$ by a neural network instead of using a reduced order model to obtain $u_N(\mu)$ and computing $\mathcal{J}(\mu)\approx J(u_N(\mu),\mu)$ afterwards.\newline
	\structure{Usage shown in the tutorial:}
	\href{https://docs.pymor.org/main/tutorial_mor_with_anns.html}{\ttfamily\footnotesize https://docs.pymor.org/main/tutorial\_mor\_with\_anns.html}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
	\frametitle{All available neural network-based reductors}
	\structure{Stationary problems:}
	\begin{itemize}
		\item \mintinline[fontsize=\small]{python}{NeuralNetworkReductor}\newline
		{Approximates map from parameter to reduced state.}
		\item \mintinline[fontsize=\small]{python}{NeuralNetworkStatefreeOutputReductor}\newline
		{Approximates map from parameter to output. *}
	\end{itemize}
	\vspace{0.3cm}
	\structure{Instationary problems:}
	\begin{itemize}
		\item \mintinline[fontsize=\small]{python}{NeuralNetworkInstationaryReductor}\newline
		{Approximates map from parameter and time to reduced state.}
		\item \mintinline[fontsize=\small]{python}{NeuralNetworkInstationaryStatefreeOutputReductor}\newline
		{Approximates map from parameter and time to output. *}
	\end{itemize}
	* \emph{New in release 2021.1!}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
	\frametitle{Remarks on the method}
	\begin{minipage}[t]{.4\textwidth}
		\textcolor{green}{Advantages:}
		\begin{itemize}
			\item Non-intrusive method.
			\item Parameter separability is not required.
			\item Very fast during online computations.
			\item Orthogonal projection produces smaller error than Galerkin projection (constant from the Lemma of Céa).
		\end{itemize}
	\end{minipage}
	\hspace{0.6cm}
	\vrule
	\hspace{0.6cm}
	\begin{minipage}[t]{.4\textwidth}
		\textcolor{red}{Disadvantages:}
		\begin{itemize}
			\item Neural network produces additional error (beside the error of the reduced space).
			\item Finding a proper neural network architecture is crucial to obtain good results.
			\item Training in the offline phase might be expensive.
		\end{itemize}
	\end{minipage}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{}
\begin{frame}[fragile]
	\frametitle{References}
	\begin{thebibliography}{99}
		\bibitem{hesthaven1}Jan S. Hesthaven and Stefano Ubbiali: Non-intrusive reduced order modeling of non-linear problems using neural networks.
		\newblock \textit{J. Comput. Phys.}, 363:55-78, 2018.
		
		\bibitem{hesthaven2}Qian Wang, Jan S. Hesthaven, and Deep Ray: Non-intrusive reduced order modeling of unsteady flows using artificial neural networks with application to a combustion problem.
		\newblock \textit{J. Comput. Phys.}, 384:289-307, 2019.
	\end{thebibliography}
\end{frame}
\plain{Are there questions?}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End: